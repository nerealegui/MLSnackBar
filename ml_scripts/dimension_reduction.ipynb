{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) on Employee_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Principal Component Analysis (PCA) is to reduce the **number of dimensions** of a d-dimensional dataset by projecting it onto a k-dimensional subspace (with k < d) in order to increase the **computational efficiency** while retaining most of the information.\n",
    "\n",
    "The k dimensions that we keep (eigenvectors) are called \"**principal components**\".\n",
    "\n",
    "The PCA approach requires to:\n",
    "\n",
    "* Standardize the data.\n",
    "* Obtain the Eigenvectors and Eigenvalues from a Singular Vector Decomposition (SVD).\n",
    "* Choose the number k of principal components to keep.\n",
    "* Construct a projection matrix with the selected k eigenvectors.\n",
    "* Project original dataset to a k-dimensional feature subspace.\n",
    "\n",
    "Choosing the number k can be done systematically by selecting the components that best describe the variance in our data. The amount of information (variance) contained by each eigenvector can be measured by the **explained variance**.\n",
    "\n",
    "This notebook will display the explained variance for your dataset and help you choose the right amount of eigenvectors (\"principal components\").\n",
    "\n",
    "* [Setup and loading the data](#setup)\n",
    "* [Preprocessing of the data](#preprocessing)\n",
    "* [Computation of the PCA](#pca)\n",
    "* [Display of the explained variance](#explained-variance)\n",
    "* [Retaining of the most significant components](#final-pca)\n",
    "* [Visualizing the vectors in the original space](#original-space)\n",
    "\n",
    "<center><strong>Select Cell > Run All to execute the whole analysis</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and dataset loading <a id=\"setup\" /> \n",
    "\n",
    "First of all, let's load the libraries that we'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import sys                          \n",
    "import pandas as pd, numpy as np             # Data manipulation \n",
    "from sklearn.decomposition import PCA        # The main algorithm\n",
    "from matplotlib import pyplot as plt         # Graphing\n",
    "import seaborn as sns                        # Graphing\n",
    "from collections import defaultdict, Counter # Utils\n",
    "sns.set(style=\"white\")                       # Tuning the style of charts\n",
    "import warnings                              # Disable some warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is now to load the dataset and put aside the three main types of columns:\n",
    "\n",
    "* Numerics\n",
    "* Categorical\n",
    "* Dates\n",
    "\n",
    "Since analyzing PCA requires to have the data in memory, we are only going to load a sample of the data. Modify the following cell to change the size of the sample.\n",
    "\n",
    "Also, by default, date features are not kept. Modify the following cell to change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_limit = 10000\n",
    "keep_dates = False\n",
    "\n",
    "# load the data file within a Pandas dataframe (change the dataset name for a different project)\n",
    "df = pd.read_excel('Employee_data.xlsx', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the original dataframe\n",
    "df_orig = df.copy()    \n",
    "\n",
    "# dropping now some non relevant fields\n",
    "#df = df.drop([\"gender\", \"minority\", \"jobtime\"], axis=1)\n",
    "\n",
    "# Get the column names\n",
    "numerical_columns = list(df.select_dtypes(include=[np.number]).columns)\n",
    "categorical_columns = list(df.select_dtypes(include=[object]).columns)\n",
    "date_columns = list(df.select_dtypes(include=['<M8[ns]']).columns)\n",
    "\n",
    "# Print a quick summary of what we just loaded\n",
    "print(\"Loaded dataset\")\n",
    "print(\"   Rows: %s\" % df.shape[0])\n",
    "print(\"   Columns: %s (%s num, %s cat, %s date)\" % (df.shape[1], \n",
    "                                                    len(numerical_columns), len(categorical_columns),\n",
    "                                                    len(date_columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the data <a id=\"preprocessing\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the dates as features if requested by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "if keep_dates:\n",
    "    df[date_columns] = df[date_columns].astype(int)*1e-9\n",
    "else:\n",
    "    columns_to_drop.extend(date_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of the columns that contain too many unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_LIMIT_ABS = 200\n",
    "CAT_DROP_LIMIT_RATIO = 0.5\n",
    "for feature in categorical_columns:\n",
    "    nu = df[feature].nunique()\n",
    "    \n",
    "    if nu > DROP_LIMIT_ABS or nu > CAT_DROP_LIMIT_RATIO*df.shape[0]:\n",
    "        print(\"Dropping feature %s with %s values\" % (feature, nu))\n",
    "        columns_to_drop.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to impute missing values (or drop the records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "impute = True\n",
    "\n",
    "if impute:\n",
    "    # Use mean for numerical features\n",
    "    for feature in numerical_columns:\n",
    "        v = df[feature].mean()\n",
    "        if np.isnan(v):\n",
    "            v = 0\n",
    "        print(\"Filling %s with %s\" % (feature, v))\n",
    "        df[feature] = df[feature].fillna(v)\n",
    "\n",
    "    # Use mode for categorical features\n",
    "    for feature in categorical_columns:\n",
    "        v = df[feature].value_counts().index[0]\n",
    "        df[feature] = df[feature].fillna(v)\n",
    "\n",
    "else:        \n",
    "    # drop records\n",
    "    df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dropping the following columns: %s\" % columns_to_drop)\n",
    "df = df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all categorical features, we are going to \"dummy-encode\" them (also sometimes called one-hot encoding).\n",
    "\n",
    "Basically, a categorical feature is replaced by one column per value. Each created value contains 0 or 1 depending on whether the original value was the one of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical variables with more than that many values, we only keep the most frequent ones\n",
    "LIMIT_DUMMIES = 100\n",
    "\n",
    "# Only keep the top 100 values\n",
    "def select_dummy_values(train, features):\n",
    "    dummy_values = {}\n",
    "    for feature in features:\n",
    "        values = [\n",
    "            value\n",
    "            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n",
    "        ]\n",
    "        dummy_values[feature] = values\n",
    "    return dummy_values\n",
    "\n",
    "DUMMY_VALUES = select_dummy_values(df, [x for x in categorical_columns if not x in columns_to_drop])\n",
    "\n",
    "\n",
    "def dummy_encode_dataframe(df):\n",
    "    for (feature, dummy_values) in DUMMY_VALUES.items():\n",
    "        for dummy_value in dummy_values:\n",
    "            if sys.version_info > (3,0):\n",
    "                dummy_name = '%s_value_%s' % (feature, dummy_value)\n",
    "            else:\n",
    "                dummy_name = u'%s_value_%s' % (feature, dummy_value.decode('utf-8'))\n",
    "            df[dummy_name] = (df[feature] == dummy_value).astype(float)\n",
    "        del df[feature]\n",
    "        print('Dummy-encoded feature %s' % feature)\n",
    "\n",
    "dummy_encode_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap to show correlation between explanatory variables\n",
    "sns.set(font_scale=1.1)\n",
    "fig, ax = plt.subplots(figsize=(8,8))         # Sample figsize in inches\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\", linewidths=1, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we rescale the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler().fit(df)\n",
    "X_std = ss.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the PCA <a id=\"pca\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's \"fit\" the PCA algorithm (in other words, let's compute the singular value decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_pca = PCA()\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the PCA is a full SVD (k=d, we have not yet applied any \"reduction\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display of the explained variance of the eigenvectors. <a id=\"explained-variance\" />\n",
    "\n",
    "The first thing to do after fitting a PCA algorihtm is to plot the **explained variance** of each eigenvector (how much information from the original data does each vector contain).\n",
    "\n",
    "We also compute how many of these vectors (in order) must be used to retain 90% of the variance of the original dataset (you can change that figure below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(sklearn_pca.n_components_), sklearn_pca.explained_variance_ratio_, alpha=0.5, align='center',label='individual explained variance')\n",
    "plt.step(range(sklearn_pca.n_components_), [sklearn_pca.explained_variance_ratio_[:y].sum() for y in range(1,sklearn_pca.n_components_+1)], alpha=0.5, where='mid',label='cumulative explained variance')\n",
    "plt.axhline(y=0.95, linewidth=2, color = 'r')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.xlim([0, sklearn_pca.n_components_])\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot of explained_variance\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "ax.bar(\n",
    "    x      = np.arange(sklearn_pca.n_components_) + 1,\n",
    "    height = sklearn_pca.explained_variance_ratio_\n",
    ")\n",
    "\n",
    "for x, y in zip(np.arange(len(df.columns)) + 1, sklearn_pca.explained_variance_ratio_):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (x,y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "ax.set_xticks(np.arange(sklearn_pca.n_components_) + 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Feature Explained Variance')\n",
    "ax.set_xlabel('PCA Component')\n",
    "ax.set_ylabel('% Explained Variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show data from the chart\n",
    "print (\"Component explained variance:\")\n",
    "variances = pd.DataFrame(sklearn_pca.explained_variance_ratio_, columns=['Variances'])\n",
    "variances_cum = pd.DataFrame(sklearn_pca.explained_variance_ratio_.cumsum(), columns=['Cumulative'])\n",
    "frames = [variances, variances_cum] \n",
    "df_var = pd.concat(frames, sort=False, axis=1)\n",
    "df_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot \n",
    "plt.plot(\n",
    "    range(1,len(sklearn_pca.explained_variance_ratio_ )+1),\n",
    "    sklearn_pca.explained_variance_ratio_,\n",
    "    c='red', marker='o'\n",
    ")\n",
    " \n",
    "plt.xlabel('PCA Component')\n",
    "plt.ylabel('% Cumulative Explained Variance')\n",
    "plt.title('Scree plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retaining only some vectors <a id=\"final-pca\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should decide now how many components you want to keep and change the following parameter.\n",
    "\n",
    "By default we keep the recommended value from the above figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANCE_TO_KEEP = 0.90\n",
    "keep_recommend = [sklearn_pca.explained_variance_ratio_[:y].sum()>VARIANCE_TO_KEEP for y in range(1,sklearn_pca.n_components_+1)].count(False)\n",
    "print(\"Number of components to keep to retain %s%% of the variance:\" % (100*VARIANCE_TO_KEEP), keep_recommend, \"out of the original\", sklearn_pca.n_components_)\n",
    "retained_components_number = keep_recommend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the PCA again but with a limited number of components this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_pca_final = PCA(n_components=retained_components_number)\n",
    "Y_sklearn_final = sklearn_pca_final.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the eigenvectors in the original feature space <a id=\"original-space\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our eigenvectors has a linear decomposition in the original feature space.\n",
    "\n",
    "To understand which features were the most important, we can see how our eigenvectors are made of each original feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows loading factors = correlations between variables and factors\n",
    "print (\"Loading factors:\")\n",
    "\n",
    "dfloading=pd.DataFrame(\n",
    "    data    = sklearn_pca_final.components_,\n",
    "    columns = df.columns,\n",
    ")\n",
    "\n",
    "index_list=[]\n",
    "for n in range(retained_components_number):\n",
    "    index_list.append(\"PC\"+str(n+1))\n",
    "\n",
    "dfloading.index = index_list\n",
    "dfloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each variable can be expressed as a lineal combination of Factors. For instance:\n",
    "educ = 0.452923*PC1 -0.125929*PC2 -0.042881*PC3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we hide values lower than 0.2 or 0.3 to display better the relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide values so that it is clearer\n",
    "dfloading2 = dfloading.where(abs(dfloading) > 0.3  , \"\")\n",
    "dfloading2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For display reasons, we don't show all components if more than 50 (same for input variables)\n",
    "n_components_to_show = min(50, sklearn_pca_final.n_components_)\n",
    "n_input_features = sklearn_pca_final.components_.shape[1]\n",
    "\n",
    "decomp_df = pd.DataFrame(sklearn_pca_final.components_[0:n_components_to_show],\n",
    "                            columns=df.columns)\n",
    "if decomp_df.shape[1] > 50:\n",
    "    decomp_df = decomp_df[decomp_df.columns[0:50]]\n",
    "\n",
    "fig = plt.figure(figsize=(n_input_features, n_components_to_show))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(decomp_df, square=True, annot=True).set(title='Component Matrix: Loading factors')\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communalities\n",
    "In order to calculate communalities (% of information explained from each variable), we need to square the loading factors of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communalities\n",
    "commun=[]\n",
    "\n",
    "for col in dfloading.columns:\n",
    "    commun.append((dfloading.iloc[0:3][col]**2).sum())\n",
    "    \n",
    "dfcommun = pd.DataFrame(commun,index=dfloading.columns, columns=[\"Communalities\"])\n",
    "dfcommun.sort_values(\"Communalities\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the loadings of x and y axes\n",
    "dfloadT = dfloading.T\n",
    "\n",
    "xs = dfloadT.PC1\n",
    "ys = dfloadT.PC2\n",
    " \n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.axvline(x = 0, color = 'b', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'b', linestyle = 'dashed')\n",
    "\n",
    "# Plot the loadings on a scatterplot\n",
    "for i, varnames in enumerate(dfloading.columns):\n",
    "    plt.scatter(xs[i], ys[i], s=50)\n",
    "    plt.text(xs[i], ys[i], \"  \" + varnames)\n",
    " \n",
    "# Define the axes\n",
    "xticks = np.linspace(-0.8, 0.8, num=5)\n",
    "yticks = np.linspace(-0.8, 0.8, num=5)\n",
    "plt.xticks(xticks)\n",
    "plt.yticks(yticks)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    " \n",
    "# Show plot\n",
    "plt.title('Component plot')\n",
    "plt.savefig('Component plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing projected vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of two components to display them\n",
    "sklearn_pca_2 = PCA(n_components=2)\n",
    "Y_sklearn_2 = sklearn_pca_2.fit_transform(X_std)\n",
    "indx=np.arange(1, Y_sklearn_2.shape[0]+1)\n",
    "\n",
    "pca_df_2 = pd.DataFrame(\n",
    "    data=Y_sklearn_2, \n",
    "    columns=['PC1', 'PC2'])\n",
    "\n",
    "sns.set()\n",
    "sns.lmplot(\n",
    "    x='PC1', \n",
    "    y='PC2', \n",
    "    data=pca_df_2, \n",
    "    fit_reg=False, \n",
    "    legend=True\n",
    "    )\n",
    " \n",
    "plt.title('2D PCA Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now it shows the data Projections\n",
    "pca_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original data with components created\n",
    "frames = [df_orig, pca_df_2]\n",
    "dffinal = pd.concat(frames,axis=1)\n",
    "dffinal.to_excel(\"Final.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "\n",
    "n_comps = 2\n",
    "\n",
    "methods = [\n",
    "    (\"PCA\", PCA()),\n",
    "    (\"Unrotated FA\", FactorAnalysis()),\n",
    "    (\"Varimax FA\", FactorAnalysis(rotation=\"varimax\")),\n",
    "]\n",
    "\n",
    "num_methods = np.arange(3)\n",
    "\n",
    "for ax, (method, fa) in zip(num_methods, methods):\n",
    "    fa.set_params(n_components=n_comps)\n",
    "    ss = StandardScaler().fit(df)\n",
    "    X_std = ss.transform(df)\n",
    "    Y_sklearn = fa.fit_transform(X_std)\n",
    "    \n",
    "    components = fa.components_.T\n",
    "    if method == \"Unrotated FA\":\n",
    "        df_factor = pd.DataFrame(components, columns = ['PC1','PC2'], index=df.columns)\n",
    "    elif method == \"Varimax FA\":\n",
    "        df_varimax = pd.DataFrame(components, columns = ['PC1','PC2'], index=df.columns)\n",
    "    else:\n",
    "        df_pca = pd.DataFrame(components, columns = ['PC1','PC2'], index=df.columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA: \")\n",
    "df_pca.where(abs(df_pca) > 0.3  , \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unrotated Factor: \")\n",
    "df_factor.where(abs(df_factor) > 0.3  , \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Varimax rotation: \")\n",
    "df_varimax.where(abs(df_varimax) > 0.3  , \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent the component plot rotated\n",
    "xs = df_varimax.PC1\n",
    "ys = df_varimax.PC2\n",
    " \n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.axvline(x = 0, color = 'b', linestyle = 'dashed')\n",
    "plt.axhline(y = 0, color = 'b', linestyle = 'dashed')\n",
    "\n",
    "# Plot the loadings on a scatterplot\n",
    "for i, varnames in enumerate(df_varimax.T):\n",
    "    plt.scatter(xs[i], ys[i], s=50)\n",
    "    plt.text(xs[i], ys[i], \"  \" + varnames)\n",
    " \n",
    "# Define the axes\n",
    "xticks = np.linspace(-0.8, 0.8, num=5)\n",
    "yticks = np.linspace(-0.8, 0.8, num=5)\n",
    "plt.xticks(xticks)\n",
    "plt.yticks(yticks)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    " \n",
    "# Show plot\n",
    "plt.title('Component plot varimax rotation')\n",
    "plt.savefig('Component plot varimax rotation.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "analyzedDataset": "Employee_data",
  "createdOn": 1689353912399,
  "creator": "alvaro.mendez@uam.es",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "modifiedBy": "alvaro.mendez@uam.es",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
