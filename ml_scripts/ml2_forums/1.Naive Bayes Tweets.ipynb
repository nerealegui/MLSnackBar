{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\")) # Increase cell width\n",
    "display(HTML(\"<style>.rendered_html { font-size: 16px; }</style>\")) # Increase font size\n",
    "\n",
    "# Larger figures\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Welcome to the first application of Probabilistic classification with Naïve Bayes!\n",
    "\n",
    "<html>\n",
    "<img src=\"https://www.sketchappsources.com/resources/source-image/twitterlogo_1x.png\" width=\"20%\">\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1\n",
    "In this example we will use **Twitter** as our data source to filter those tweets that are talking about a given application. This is a typical problem in probabilistic classification, where I'll use a large sample of texts corresponding to the category that I want to recognize, and another large sample of texts unrelated to that category. That way, by exploring the different word frequencies and probabilities, we'll determine if a new text belongs to one or another category, by simply looking at the existing evidence.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "There is a real app called \"Mandrill\"\n",
    "\n",
    "<html>\n",
    "<img src=\"https://pipedream.com/s.v0/app_mqehAz/logo/orig\" width=\"15%\"><P>\n",
    "</html>\n",
    "\n",
    "And I want to scan twitter to capture only those tweets that mention my APP. But I don't want to read tweets talking about the animal (the actual mandrill), so I need a classifier for the tweets, that will **filter** only those which are relevant.\n",
    "\n",
    "For this part of the problem part of the data preparation job is already done, so you start with a few hundreds tweets captured using Twitter API, with the word **Mandrill** in them. The file with tweets (`appWords.txt`) referring to the app looks like this:\n",
    "\n",
    "    @ericcandino they're unfortunately not for sale but drop us a line via http://help.mandrill.com  a\n",
    "    @gidogeek you can see what we've been working on and get a general idea of our plans at http://blo\n",
    "    @guillaumepotier there are several reasons emails go to spam mind submitting a request at http://h\n",
    "    @icntmx yep  we'd be glad to would you mind submitting a request at http://help.mandrill.com\n",
    "    @jeremyweir if you submit a request at http://help.mandrill.com   we'll get back to you with some\n",
    "    @josscrowcroft mind submitting a request via http://help.mandrill.com  with some additional detail\n",
    "\n",
    "And the file with tweets (`otherWords.txt`) not talking about the app look like this:\n",
    "\n",
    "    anyway  yeah  that's a thing that's going on  reincarnated mandrill-men\n",
    "    arin did the spark mandrill trick i was wondering if he would :')\n",
    "    audio mandrill - happy beat this is a funk song by a band who liked to w\n",
    "    cannot believe i am the only one in a @mandrill 2012 #tweetfleet t-shirt\n",
    "    chill penguin and spark mandrill down #megamanx\n",
    "    cuando pase el bafici y se hayan perdido mandrill  mirageman  mujer metr\n",
    "    de los creadores de #kiltro #mirageman y #mandrill ahora atacan con #trá\n",
    "\n",
    "I trimmed lines for better representation, but they're arbitrarily long (within twitter limits).\n",
    "\n",
    "As you might probably have realized, this is a **supervised problem**, and the _labeling_ of the training data has been already done, by manually separating the tweets among the two possible sets. That is the most boring part, and you always need to do so to train any classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "What I did to prepare the problem is to process the tweets to convert _raw_ two data files with the frequency count for each individual word on them. So, from `appWords.txt`, I generated `appFreqs.csv`, which summary is like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appFile = pd.read_csv(\"data/appFreqs.csv\", header=None, names=[\"word\", \"frequency\"])\n",
    "otherFile = pd.read_csv(\"data/otherFreqs.csv\", header=None, names=[\"word\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files contains a list of words and their frequency. We need to compute the probabilities from these frequencies (i.e., number of times a word appears w.r.t. the total count of words).\n",
    "\n",
    "To that end, I did simply count the number of occurrences of each word (`frequency`), divided by the sum of occurrences of all the words, and put that in the column variable `probability`, but I also computed the $log$ of the probability. Remember the we can use the actual probability as:\n",
    "\n",
    "$$ P(word) = \\frac{count(word)}{\\sum_{i=1}^{N}count(word_{i})} $$\n",
    "\n",
    "or the $log(P)$, as it is more convenient to use those values than the tiny ones that the probability produces (a product of tiny values produces an even more tiny one). Remember that when using $logs$ we must sum them, instead of multiplying them. So, what we have in the variable `probability` is:\n",
    "\n",
    "$$ logP(word) = log \\left( \\frac{count(word)}{\\sum_{i=1}^{N}count(word_{i})} \\right)  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appTotal = sum(appFile.frequency)\n",
    "otherTotal = sum(otherFile.frequency)\n",
    "\n",
    "appFile['probability'] = appFile.frequency.apply(lambda x: math.log(x/appTotal))\n",
    "otherFile['probability'] = otherFile.frequency.apply(lambda x: math.log(x/otherTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are now charting the frequencies of both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as  mpatches\n",
    "\n",
    "long = appFile['frequency']\n",
    "short = otherFile['frequency']\n",
    "sns.kdeplot(appFile['frequency'], color=\"red\", shade=True, clip=(0,500)).set(title='Density plots for Word frequency in app-related tweets (red) and non app related tweets(blue)')\n",
    "sns.kdeplot(otherFile['frequency'], color=\"blue\", shade=True, clip=(0,500))\n",
    "handles = [mpatches.Patch(facecolor=plt.cm.Reds(100), label=\"App\"),\n",
    "           mpatches.Patch(facecolor=plt.cm.Blues(100), label=\"Other\")]\n",
    "plt.legend(handles=handles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the words frequencies obtained for the two sets are quite similar. This doesn’t mean that it will be impossible to differentiate one class from the other. This simply means that the frequencies in both sets correspond to a similar communication pattern (tweets in english, mostly). <BR>Actually, I cutted the X axis at 500 but there are around 3000 different words on each set, but the frequencies of the long-tail part are really small.<BR>\n",
    "To classify between the two possibilities, we need to look at the words present in the new tweets, and see where are more frequent among the two distributions. <BR>Let’s go for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need a function gives me the probability of any given word in the data frames that I used for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_w_prob(word, dataframe):\n",
    "    prob = dataframe[dataframe.word == word].probability\n",
    "    if len(prob) > 0:\n",
    "        return prob.values[0]\n",
    "    else:\n",
    "        return math.log(1/math.log(sum(dataframe.frequency)))\n",
    "    \n",
    "# for instance:\n",
    "np.exp(get_w_prob(\"#al\", appFile)) #inverse of the log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also need to compute the **prior probability** of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appPrior = math.log(len(appFile) / (len(appFile) + len(otherFile))) # Number of Tweets in app File / Total number of Tweets (sum of the number of tweets in both files)\n",
    "print(\"The prior of tweets belonging to the app is = {0:.2f}\".format(math.exp(appPrior)))\n",
    "\n",
    "otherPrior = math.log(len(otherFile) / (len(appFile) + len(otherFile))) # Number of Tweets in other File / Total number of Tweets (sum of the number of tweets in both files)\n",
    "print(\"The prior of tweets NOT belonging to the app is = {0:.2f}\".format(math.exp(otherPrior)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bayesian classifier. \n",
    "\n",
    "Let's build the classifier. I'm using a test set with a few tweets (`test.csv`), and the goal is to read them and say if they are about the APP or not. The test set is already labeled with the class each belongs to in the first column. We will loose that information to check if our prediction is OK.\n",
    "\n",
    "Read a test file, with the category label in V1 and the tweet contents in V2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\", header = None, names=[\"label\", \"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's loop through the file to compute the **MAP (maximum A Posterior Probability)** and thus, determine which class the tweet belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "\n",
    "for j in range(0,len(test)):\n",
    "    tweet = test.iloc[j].content # Extract the content of the tweet\n",
    "    print(\"Processing tweet:\",tweet)\n",
    "    wordsInThisTweet = ' '.join(tweet.split()).strip().split(\" \") # Extract the words into a list\n",
    "    appProb = 0.0\n",
    "    otherProb = 0.0\n",
    "    \n",
    "    # For every word in this tweet, sum its frequency value by using the function defined a few cells above\n",
    "    for word in wordsInThisTweet:\n",
    "        appProb = appProb + get_w_prob(word, appFile)\n",
    "        otherProb = otherProb + get_w_prob(word, otherFile)\n",
    "    posteriorAppPob = appProb * appPrior\n",
    "    posteriorOtherPob = otherProb * otherPrior\n",
    "    \n",
    "    # Categorize according to the score obtained from every subset (App tweets, and Other tweets)\n",
    "    if posteriorAppPob > posteriorOtherPob:\n",
    "        print(\"Prediction = {} | Actual Label= {}\".format(\"APP\", test.iloc[j].label))\n",
    "        print()\n",
    "        pred.append(\"APP\")\n",
    "    else:\n",
    "        print(\"Prediction = {} | Actual Label= {}\".format(\"OTHER\", test.iloc[j].label))\n",
    "        print()\n",
    "        pred.append(\"OTHER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sn\n",
    "print(\"Accuracy = {}\".format(accuracy_score(test.label, pred)))\n",
    "print()\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "cm = confusion_matrix(test.label, pred)\n",
    "sn.heatmap(cm, annot=True, cmap=plt.cm.Blues);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good 80% of Accuracy with this simple implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
