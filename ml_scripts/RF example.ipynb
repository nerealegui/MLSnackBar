{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas_summary\n",
    "# !pip install waterfallcharts\n",
    "# !pip install treeinterpreter\n",
    "# !pip install time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import waterfall_chart\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.tabular import *\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, precision_recall_curve, confusion_matrix, recall_score, precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib.ticker import FuncFormatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.c_[cancer.data, cancer.target]\n",
    "columns = np.append(cancer.feature_names, [\"target\"])\n",
    "cancer_df=pd.DataFrame(data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show class distribution of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(cancer_df[\"target\"], return_counts=True)\n",
    "plt.pie(counts, labels=unique, autopct='%.0f%%');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 569 rows and 31 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "cancer_df.columns[cancer_df.isnull().any()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest first try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(cancer_df.iloc[:,cancer_df.columns != 'target'],\n",
    "                                                    cancer_df['target'],\n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the fist RF classifier only to find later optimal parameters <BR>(this would be just Bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseRF = RandomForestClassifier(n_estimators = 200,  #The number of trees in the forest\n",
    "                               random_state = 0,\n",
    "                               n_jobs = -1, # The number of jobs to run in parallel (-1 means all processors, 1 no parallelism)\n",
    "                               oob_score = True) # Whether to use out-of-bag samples to estimate the generalization score (increases time)\n",
    "# oob are the samples not chosen on the boostrapping process\n",
    "\n",
    "# other parameters:\n",
    "# criterion{\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
    "# max_depth: int, default=None\n",
    "# min_samples_leaf: int or float, default=1 - The minimum number of samples required to be at a leaf node\n",
    "# min_samples_split: int or float, default=2 - The minimum number of samples required to split an internal node:\n",
    "# max_features{\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\" - The number of features to consider when looking for the best split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are searching for the set of parameters that give the best performance, then we will need to use them in a RF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'min_samples_leaf' :[1,3,5], \n",
    "    'max_features' : [10,15,20,25,30],\n",
    "    'criterion' : ['gini','entropy'] #,'log_loss']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following execution will take some time (1 to 2 minutes). Ignore the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "cvRF = GridSearchCV(estimator=baseRF, param_grid=param_grid, cv=3, scoring='roc_auc') \n",
    "#cv is the number of cross validation iterations to be performed\n",
    "cvRF.fit(x_train,y_train)\n",
    "\n",
    "print (\"Completed in --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the best combination of criteria found\n",
    "cvRF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the best score (of the scoring function we selected, here ROC AUC) from TRAINING DATASET\n",
    "cvRF.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best parameters are {'criterion': 'entropy', 'max_features': 15, 'min_samples_leaf': 5}.<BR>\n",
    "Our average AUC score of all folds for training dataset is 0.989."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we train the model with the best parameters and using Random Forest instead of simple Bagging.\n",
    "##### The difference is the parameter \"max_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 200, \n",
    "                               random_state = 0,\n",
    "                               max_features = 15, #this parameter makes the difference between simple Bagging and Random Forests\n",
    "                               n_jobs = -1,\n",
    "                               oob_score = True,\n",
    "                               criterion = 'entropy',\n",
    "                               min_samples_leaf = 5)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "# predictions\n",
    "y_pred_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_pred_test, return_counts=True)\n",
    "plt.pie(counts, labels=unique, autopct='%.0f%%');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OOB score: {:.3f}\".format(model.oob_score_))\n",
    "# The OOB score is an estimate of the generalization error of the model, \n",
    "# calculated with the data not used to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes of the model object (for your reference)\n",
    "# print(dir(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are applying our model to the TEST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = model.predict_proba(x_test) #probability of being in both groups\n",
    "prob_malign = [p[1] for p in pred_prob] #probability of being in the malign group\n",
    "auc = roc_auc_score(y_test, prob_malign)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC score: {:.3f}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our auc score is very good for this model. <BR>\n",
    "Let's compare accuracy and show ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC Calculations - false positive rates, true positive rates and thresholds\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, prob_malign, pos_label=1)\n",
    "\n",
    "train_acc = round(model.score(x_train,y_train) * 100,2) #Train Accuracy score\n",
    "test_acc = round(model.score(x_test,y_test) * 100,2) #Test Accuracy score\n",
    "print(\"Train Accuracy score: \", train_acc, \"%\")\n",
    "print(\"Test Accuracy score: \", test_acc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(fpr, tpr, label=\"ROC\")\n",
    "ax.plot([0, 1], [0, 1],linestyle='--', label=\"Random model\") \n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "ax.set_box_aspect(1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the Precision and Recall chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compare Recall and Precision\n",
    "\n",
    "prob_pred = model.predict_proba(x_test)[:, 1]\n",
    "thresholds = np.arange(0.0, 1.0, step=0.01)\n",
    "recall_scores = [metrics.recall_score(y_test, prob_pred > t) for t in thresholds]\n",
    "precis_scores = [metrics.precision_score(y_test, prob_pred > t) for t in thresholds]\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(thresholds, recall_scores, label=\"Recall @ t\")\n",
    "ax.plot(thresholds, precis_scores, label=\"Precision @ t\")\n",
    "ax.axvline(0.5, c=\"gray\", linestyle=\"--\", label=\"Default Threshold\")\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Metric @ Threshold\")\n",
    "ax.set_box_aspect(1)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree interpreter: calculate the contribution of each feature to the tree prediction.\n",
    "##### We will use it first with scikit and then with an additional library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)\n",
    "model.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "\n",
    "indexes = np.argsort(importances)[::-1]\n",
    "sorted_imp = importances[indexes]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(len(importances)), importances[indexes], align='center')\n",
    "plt.xticks(range(len(importances)), np.array(model.feature_names_in_)[indexes], rotation=90)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Feature importances\")\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),\n",
    "             f'{sorted_imp[i]:.2f}', ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now with an additional library that shows individual importance directions and decomposes the model prediction into three parts: \n",
    "1. Bias: The overall average prediction of the model without taking individual features into account.\n",
    "2. Contributions: The specific influence of each feature on the prediction value for the instance in question.\n",
    "3. Final Prediction: This is the final result for the particular observation, obtained by summing the bias and all feature contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treeinterpreter import treeinterpreter as ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, biases, contributions = ti.predict(model, x_test.values)\n",
    "\n",
    "# Convert contributions into a dataframe\n",
    "contributions_df = pd.DataFrame(contributions[:, :, 0], columns=x_test.columns)\n",
    "\n",
    "# Suming contributions from every row\n",
    "total_contributions = contributions_df.sum(axis=0)\n",
    "\n",
    "# Show chart (now displaying features with contribution less than 0.03)\n",
    "waterfall_chart.plot(x_test.columns, total_contributions, rotation_value=90, threshold=0.03, formatting='{:,.2f}')\n",
    "plt.title('Waterfall Chart of Total Contributions')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Contributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the optimum number of trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting does not depend on the number of trees, but we don´t want to run with more trees than necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "cv_scores    = []\n",
    "\n",
    "estimator_range = range(5, 105, 5) #from 5 trees to 100, step 5\n",
    "\n",
    "for n_estimators in estimator_range:\n",
    "    model = RandomForestClassifier(\n",
    "                n_estimators = n_estimators,\n",
    "                max_features = 15, # the previous optimum\n",
    "                oob_score    = False,\n",
    "                n_jobs       = -1, \n",
    "                random_state = 123\n",
    "             )\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    acc=accuracy_score(y_test, predictions)\n",
    "    print(\"% Accuracy of test dataset for {} trees is {:.3f}\".format(n_estimators, acc))\n",
    "    train_scores.append(acc) # storing accuracy from each iteration\n",
    "\n",
    "    # and we also store the accuracy mean obtained from running a 5-fold validation\n",
    "    cvscores = cross_val_score(\n",
    "                estimator = model,\n",
    "                X         = x_train,\n",
    "                y         = y_train,\n",
    "                scoring   = 'accuracy',\n",
    "                cv        = 5\n",
    "             )\n",
    "    cv_scores.append(cvscores.mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(estimator_range, train_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title('Accuracy of Test Dataset vs. Number of Trees')\n",
    "plt.xlabel('Number of Trees (n_estimators)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(estimator_range)  # Para mostrar todos los valores en el eje x\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "ax.plot(estimator_range, cv_scores, label=\"cv scores\")\n",
    "ax.plot(estimator_range[np.argmax(cv_scores)], max(cv_scores),\n",
    "        marker='o', color = \"red\", label=\"max score\")\n",
    "ax.set_ylim(0.93, 0.97)\n",
    "ax.set_ylabel(\"% Accuracy\")\n",
    "ax.set_xlabel(\"n_estimators\")\n",
    "ax.set_title(\"Evolution of cv-error vs. number of trees\")\n",
    "plt.legend();\n",
    "print(f\"Optimum number of trees: {estimator_range[np.argmax(cv_scores)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
